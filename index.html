<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3D Vision and Modeling Challenges in eCommerce</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        header {
            text-align: center;
            padding: 20px 0;
            border-bottom: 1px solid #e0e0e0;
        }
        .workshop-title {
            font-size: 24px;
            margin-bottom: 10px;
        }
        .workshop-subtitle {
            font-size: 18px;
            margin-bottom: 10px;
        }
        .workshop-date {
            font-size: 16px;
            margin-bottom: 20px;
        }
        .section {
            margin-bottom: 30px;
        }
        .section-title {
            font-size: 20px;
            margin-bottom: 15px;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 5px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 20px;
        }
        th, td {
            border: 1px solid #e0e0e0;
            padding: 10px;
            text-align: left;
        }
        th {
            background-color: #f0f0f0;
        }
        .call-for-papers, .submission, .important-dates, .publication, .workshop-chairs, .program-committee, .short-cv, .invited-speakers {
            margin-bottom: 30px;
        }
        .topic-list, .submission-requirements {
            list-style-type: disc;
            padding-left: 20px;
        }
        .topic-list li, .submission-requirements li {
            margin-bottom: 5px;
        }
        .reference {
            margin-top: 20px;
        }
        .reference-item {
            margin-bottom: 10px;
        }
        footer {
            text-align: center;
            padding: 20px 0;
            border-top: 1px solid #e0e0e0;
            margin-top: 30px;
        }
        .cv-image {
            width: 100px;
            height: auto;
            border-radius: 5px;
            margin-right: 15px;
            float: left;
        }
        .cv-content {
            overflow: hidden;
        }
        .invited-speaker {
            margin-bottom: 20px;
        }
        .invited-speaker-image {
            width: 100px;
            height: auto;
            border-radius: 5px;
            margin-right: 15px;
            float: left;
        }
        .invited-speaker-content {
            overflow: hidden;
        }
        .abstract {
            font-style: italic;
            margin-top: 10px;
        }
    </style>
</head>
<body>
    <header>
        <h1 class="workshop-title">Workshop on</h1>
        <h2 class="workshop-subtitle">Visual Text Generation and Text Image Processing</h2>
        <p class="workshop-date">ICDAR 2025 Workshop<br>September 16-21, 2025 @ Wuhan, Hubei, China</p>
    </header>

    <div class="section">
        <h3 class="section-title">Introduction</h3>
        <p>V************************************************************************************************************************** pe**************************************************************************************************************************ing.</p>
        <p>V****************************************************************************************************************************************************************************************************************************************************pes and edge cases [3, 4]. Text image preprocessing [5], serving as the foundation of reliable visual text analysis pipelines, tackles real-world challenges such as low resolution [6, 7], uneven illumination [8, 9], and geometric distortions [10, 11]. These techniques are essential for handling various visual text conditions, from documents to scene images.</p>
        <p>Th**************************************************************************************************************************v**************************************************************************************************************************aims to bring together researchers and practitioners to work on these two topics and foster innovations in visual text analysis.</p>
    </div>

    <div class="section">
        <h3 class="section-title">Schedule</h3>
        <p>All times in Beijing Time (UTC+08:00)</p>
        <table>
            <tr>
                <th>Time</th>
                <th>Events</th>
            </tr>
            <tr>
                <td>13:50 - 14:00</td>
                <td>Opening Remarks</td>
            </tr>
            <tr>
                <td>14:00 - 14:40</td>
                <td>Prof. ZHOU, Wengang<br><br>Invited Talk 1 :</td>
            </tr>
            <tr>
                <td>14:40 - 15:20</td>
                <td>Prof. LIAN, Zhouhui<br><br>Invited Talk 2 : Font Synthesis via Deep Generative Models</td>
            </tr>
            <tr>
                <td>15:20 - 16:00</td>
                <td>Contributed Talks (best/ runner-up paper talks)</td>
            </tr>
            <tr>
                <td>16:00 - 16:20</td>
                <td>Coffee break</td>
            </tr>
            <tr>
                <td>16:20 - 17:40</td>
                <td>Poster or oral Session</td>
            </tr>
        </table>
    </div>

    <div class="section call-for-papers">
        <h3 class="section-title">Call for Papers</h3>
        <p>Acceptable submission topics may include but are not limited to:</p>
        <ul class="topic-list">
            <li>G**************************************************************************************************************************s</li>
            <li>L**************************************************************************************************************************n</li>
            <li>Re**************************************************************************************************************************sis</li>
            <li>**************************************************************************************************************************ing</li>
            <li>**************************************************************************************************************************sfer</li>
            <li>**************************************************************************************************************************e</li>
            <li>**************************************************************************************************************************ge</li>
            <li>**************************************************************************************************************************ion</li>
            <li>D**************************************************************************************************************************g</li>
            <li>**************************************************************************************************************************n</li>
            <li>**************************************************************************************************************************n</li>
        </ul>
    </div>

    <div class="section submission">
        <h3 class="section-title">Submission</h3>
        <p>**************************************************************************************************************************n the ICDAR 2025 official website. Paper length is limited to 15 pages (excluding references) and must comply with our double-blind review requirements:</p>
        <ul class="submission-requirements">
            <li>Remove all author identifiers (names, affiliations, etc.) from the manuscript</li>
            <li>Cite previous work in third-person format to avoid identity disclosure</li>
            <li>Omit acknowledgments section in initial submissions</li>
        </ul>
        <p>S*****************************************************************************************************************************************************************************************************************************ion to present the work. Detailed submission procedures are available on the ICDAR 2025 guidelines portal.</p>
    </div>

    <div class="section important-dates">
        <h3 class="section-title">Important Dates</h3>
        <ul>
            <li>Submission Deadline: April 15, 2025</li>
            <li>Decisions Announced: May 15, 2025</li>
            <li>Camera Ready Deadline: June 1, 2025</li>
            <li>Workshop: September 20, 2025</li>
        </ul>
    </div>

    <div class="section publication">
        <h3 class="section-title">Publication</h3>
        <p>Accepted papers will be published in the ICDAR 2025 proceedings (workshop).</p>
    </div>

    <div class="section workshop-chairs">
        <h3 class="section-title">Workshop Chairs</h3>
        <ul>
            <li>ZHOU, Yu, Nankai University, China</li>
            <li>ZENG, Gangyan, Nanjing University of Science and Technology, China</li>
            <li>XIE, Hongtao, University of Science and Technology of China, China</li>
            <li>YIN, Xu-Cheng, University of Science and Technology Beijing, China</li>
        </ul>
    </div>

    <div class="section program-committee">
        <h3 class="section-title">Program Committee Members (Alphabetical Order)</h3>
        <ul>
            <li>CHEN, Zhineng, Fudan University, China</li>
            <li>CHENG, Wentao, BNU-HKBU United International College, China</li>
            <li>FANG, Shancheng, Beijing Yuanshi Technology Company, China</li>
            <li>GAO, Liangcai, Peking University, China</li>
            <li>LIAN, Zhouhui, Peking University, China</li>
            <li>LIU, Juhua, Wuhan University, China</li>
            <li>YANG, Chun, University of Science and Technology Beijing, China</li>
            <li>WANG, Yaxing, Nankai University, China</li>
            <li>WANG, Yuxin, University of Science and Technology of China, China</li>
        </ul>
    </div>

    <div class="section short-cv">
        <h3 class="section-title">Short CV of the Workshop Chairs</h3>
        
        <div class="cv">
            <img src="https://icdar-vtg-tip.github.io/static/img/people/zhouyu.jpg" alt="Prof. ZHOU, Yu" class="cv-image">
            <div class="cv-content">
                <h4>Prof. ZHOU, Yu</h4>
                <p>*********************************************************************************************************************Nankai ********************************************************************************************************************* text processing, detection, recognition and understanding. He served as AC, SPC, and PC members of CVPR, ICCV, ECCV, NeurIPS, ICDAR, and etc, and reviewers of TPAMI, TIP, and etc. He has published over 80 papers in peer-reviewed journals and conferences including CVPR, ICCV, NeurIPS, TMM, TNNLS, and etc, and the paper PIMNet has been selected as the best paper candidate in ACM MM 2021.</p>
            </div>
        </div>
        
        <div class="cv">
            <img src="/static/img/people/xiehongtao.jpg" alt="Prof. ZENG, Gangyan" class="cv-image">
            <div class="cv-content">
                <h4>Prof. ZENG, Gangyan</h4>
                <p>Gan******************************************************************************************************************************************************************************************************************************************Institute of Information Engineering, Chinese Academy of Sciences. Her primary research interests include computer vision and multimodal intelligence, with a particular focus on the analysis and understanding of scene text. She has published over 10 papers in journals and conferences including ACM MM, AAAI, and Pattern Recognition. She has also received several rewards in the field of document analysis such as the second runner-up at the 2020 CVPR DocVQA Challenge.</p>
            </div>
        </div>
        
        <div class="cv">
            <img src="/static/img/people/xiehongtao.jpg" alt="Prof. XIE, Hongtao" class="cv-image">
            <div class="cv-content">
                <h4>Prof. XIE, Hongtao</h4>
                <p>Hon*********************************************************************************************************************ing member of the Youth Innovation Promotion Association of the Chinese Academy of Sciences. Additionally, he is a member of the Chinese Academy of Sciencesâ€™ Network Space Security Expert Group. He is engaged in research in the field of artificial intelligence and multimedia content security, including visual content recognition and inference, cross-model recognition of scene text and images, cross-modal content analysis and understanding, intelligent content generation, and security. He has published over 80 academic papers as the first or corresponding author in top international journals and conferences, including IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE-TPAMI), IEEE Transactions on Image Processing (IEEE-TIP), IEEE Transactions on Knowledge and Data Engineering (IEEE-TKDE), NeurIPS (NIPS), International Conference on Computer Vision (ICCV), and Conference on Computer Vision and Pattern Recognition (CVPR). Among these publications, four have been highly cited according to the Essential Science Indicators (ESI), three are considered hot-topic papers, and two have received the Best Paper Award at conferences.</p>
            </div>
        </div>
        
        <div class="cv">
            <img src="/static/img/people/xiehongtao.jpg" alt="Prof. YIN, Xu-Cheng" class="cv-image">
            <div class="cv-content">
                <h4>Prof. YIN, Xu-Cheng</h4>
                <p>*********************************************************************************************************************, *********************************************************************************************************************rom the University of Science and Technology Beijing, China, in 1999 and 2002, respectively, and the Ph.D. degree in pattern recognition and intelligent systems from the Institute of Automation, Chinese Academy of Sciences, in 2006. He was a visiting professor in the College of Information and Computer Sciences, University of Massachusetts Amherst, USA, for three times (Jan 2013 to Jan 2014, Jul 2014 to Aug 2014, and Jul 2016 to Sep 2016). His research interests include pattern recognition, document analysis and recognition, computer vision. He has published more than 100 academic papers (IEEE T-PAMI, IEEE T-IP, CVPR, ICDAR, etc.). From 2013 to 2019, his team had won the first place of a series of text detection and recognition competition tasks for 15 times in ICDAR Robust Reading Competition.</p>
            </div>
        </div>
    </div>

    <div class="section invited-speakers">
        <h3 class="section-title">Invited Speakers</h3>
        
        <div class="invited-speaker">
            <img src="/static/img/people/xiehongtao.jpg" alt="Prof. ZHOU, Wengang" class="invited-speaker-image">
            <div class="invited-speaker-content">
                <h4>Prof. ZHOU, Wengang</h4>
                <p>We********************************************************************************************************************* His research interests include multimedia information retrieval, computer vision, and machine gaming. He has published over 100 papers in IEEE Transactions and CCF-A ranked conferences, accumulating over 20,000 Google Scholar citations with an h-index of 64. He is a recipient of the National Natural Science Foundation of China for Excellent Young Scholars and serves as an Editorial Board Member and Lead Guest Editor for IEEE Transactions on Multimedia. He has led several major research projects, including the Major Project of the Ministry of Science and Technology and the Key Project of the National Natural Science Foundation of China Joint Fund. He has been awarded the First Prize of the Wu Wenjun Artificial Intelligence Science and Technology Progress Award, the Second Prize of the Natural Science Award by the CCIG, and the Outstanding Mentor Award by the Chinese Academy of Sciences. He has been chosen as an Outstanding Member of the CAS Youth Innovation Promotion Association and is a recipient of the Youth Talent Support Program by the China Association for Science and Technology. The Ph.D. students he supervised have received the CAS Outstanding Doctoral Dissertation Award and the CCIG Outstanding Doctoral Dissertation Award.</p>
                <h5>Title:</h5>
                <div class="abstract">Abstract:</div>
            </div>
        </div>
        
        <div class="invited-speaker">
            <img src="/static/img/people/xiehongtao.jpg" alt="Prof. LIAN, Zhouhui" class="invited-speaker-image">
            <div class="invited-speaker-content">
                <h4>Prof. LIAN, Zhouhui</h4>
                <p>**************************************************************************************************************************.</p>
                <h5>**************************************************************************************************************************</h5>
                <div class="abstract">Abstract: Font Synthesis via Deep Generative Models</div>
            </div>
        </div>
    </div>

    <div class="section reference">
        <h3 class="section-title">Reference</h3>
        <div class="reference-item">[1] **************************************************************************************************************************, 2023.</div>
        <div class="reference-item">[2] **************************************************************************************************************************.</div>
        <div class="reference-item">[3] **************************************************************************************************************************</div>
        <div class="reference-item">[4] ************************************************************************************************************************** 2024.</div>
        <div class="reference-item">[5] **************************************************************************************************************************, arXiv, 2024.</div>
        <div class="reference-item">[6] **************************************************************************************************************************24.</div>
        <div class="reference-item">[7] **************************************************************************************************************************.</div>
        <div class="reference-item">[8] **************************************************************************************************************************.</div>
        <div class="reference-item">[9] **************************************************************************************************************************3.</div>
        <div class="reference-item">[10] **************************************************************************************************************************.</div>
        <div class="reference-item">[11] **************************************************************************************************************************.</div>
    </div>

    <footer>
        <p>&copy; **************************************************************************************************************************.</p>
    </footer>

    <script>
        // Simple JavaScript for any interactive elements
        document.addEventListener('DOMContentLoaded', function() {
            console.log('ICDAR-VTG-TIP Workshop page loaded successfully.');
        });
    </script>
</body>
</html>
