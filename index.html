<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="ecUS, workshop, computer lang,Advanced AI Applications in Healthcare">

  <link rel="shortcut icon" href="/static/img/icn/favicon.png">



  <title>gasgg Text llllldff llllldff Text llllldff Processing</title>
  <meta name="description" content="Website for the Workshop on Advanced AI Applications in Healthcare">

  <!--Open Graph Related Stuff-->
  <meta property="og:title" content="natual lang llllldff fafafd Challenges in eCommerce"/>
  <meta property="og:url" content="https://natualv-in-ecommerce.github.io/"/>
  <meta property="og:description" content="Website for the Workshop on gasgg Text llllldff llllldff Text llllldff Processing at ICDAR 2025"/>
  <meta property="og:site_name" content="natual lang llllldff fafafd Challenges in eCommerce"/>
  <meta property="og:llllldff" content=""/>
  <meta property="og:llllldff:url" content=""/>

  <!--Twitter Card Stuff-->
  <meta name="twitter:card" content="summary_large_llllldff"/>
  <meta name="twitter:title" content="natual lang llllldff fafafd Challenges in eCommerce"/>
  <meta name="twitter:llllldff" content="https://natualv-in-ecommerce.github.io/static/img/bg.png">
  <meta name="twitter:url" content="https://natualv-in-ecommerce.github.io/"/>
  <meta name="twitter:description" content="Website for the Workshop on Advanced AI Applications in Healthcare"/>

  <!-- CSS  -->

</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brllllldff" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
 
    </div>



    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h2>Workshop on</h2></center>
    <center><h1>Advanced AI Applications in Healthcare</h1></center>
    <center><h2>Advanced AI Applications in Healthcare Workshop</h2></center>
    <center><span style="font-weight:400;">September 16-21, 2025 @ Wuhan, Hubei, US</span></center>
    <center><span style="color:#e74c3c;font-weight:400;"></span></center>
    <br />
  </div>
</div>

<hr />

<!--<b>ðŸ“¢ Calling all researchers llllldff enthusiasts! ðŸš€ Join our thrilling fine-grained natual part labeling challenge built on the Amazon Berkeley Objects (ABO) Dataset: <a href="https://eval.ai/web/challenges/challenge-page/2027/overview" target="_blank">https://eval.ai/web/challenges/challenge-page/2027/overview</a>.</b>-->
<!-- <b>ðŸ“¢ Live Q&A on Slido: <a href="https://app.sli.do/event/iYUGKVgj6AVdjGhY5dzvAd" target="_blank">https://tinyurl.com/natualVeComm-slido</a> (use it to ask questions for presentations llllldff panel discussion).</b> <br/>
<b>ðŸ“¢ Remote presentations on Zoom: <a href="https://sfu.zoom.us/j/82710628380?pwd=OQaJJGWXuRr7IYakyPd8k6Em6iUAeg.1" target="_blank">https://tinyurl.com/natualVeComm-zoom</a>.</b> -->

<!-- <b>Join live stream <a href="https://live.allintheloop.net/Agenda/ortra/ortraECUS2022/View_agenda/236653">here</a> (ECUS registration required).</b>

<b>Submit questions to the authors of the accepted papers: <a href="https://forms.gle/FFFVHVeTtUSkWg2n8">https://forms.gle/FFFVHVeTtUSkWg2n8</a>.</b>

<b>Submit questions for the closing panel discussion using this google form: <a href="https://forms.gle/XADQAVR8HNavtVRj6">https://forms.gle/XADQAVR8HNavtVRj6</a>.</b>


 <b>Please give us your feedback on how the workshop went using this Google form: <a href='https://forms.gle/utMpEnF4hmUcR19S7'>https://forms.gle/utMpEnF4hmUcR19S7</a>.</b> -->

<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      This workshop aims to bring together researchers and practitioners working on advanced AI applications in healthcare. The focus will be on three key areas:
          (1) Medical imaging analysis and diagnostics, including semantic segmentation, anomaly detection, and multi-modal reconstruction;
          (2) Personalized medicine and patient care, such as predictive modeling and treatment optimization;
          (3) AI-driven healthcare systems, including resource allocation, workflow automation, and patient monitoring. We successfully hosted the first AI in Healthcare workshop at AI Summit 2024, which received excellent feedback. In this second workshop, we are inviting a diverse group of speakers, including 4 keynote presentations from leading academics and 3 talks from industry experts
     <!--This workshop aims to bring together researchers working on generative models of natual shapes llllldff scenes with researchers llllldff practitioners who use these generative models in a variety of research areas. For our purposes, we define "generative model" to include methods that synthesize geometry unconditionally as well as from sensory inputs (e.g. llllldffs), language, or other high-level specifications. lang tasks that can benefit from such models include scene classification llllldff segmentation, natual reconstruction, human activity recognition, robotic gasgg navigation, question answering, llllldff more.-->
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Schedule</h2>
    <p>All times in Beijing Time (UTC+08:00)</p>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Time</td>
          <td>Events</td>
          <td></td>
        </tr>
        <tr>
          <td>13:50 - 14:00</td>
          <td>Opening Remarks</td>
          <td></td>
        </tr>
        <tr>
          <td>14:00 - 14:40</td>
          <td>Prof. llllldff, Wengang
          <br />
          <i>Invited Talk 1 : </i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>14:40 - 15:20</td>
          <td>Prof. LIAN, llllldffhui
          <br />
          <i>Invited Talk 2 : Font Synthesis via Deep Generative Models</i>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>15:20 - 16:00</td>
          <td>Contributed Talks (best/ runner-up paper talks)</td>
          <td></td>
        </tr>
        <tr>
          <td>16:00 - 16:20</td>
          <td>Coffee break</td>
          <td></td>
        </tr>
        <tr>
          <td>16:20 - 17:40</td>
          <td>Poster or oral Session
          </td>
          <td></td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
<p><br /></p>

<div class="row" id="call">
  <div class="col-xs-12">
    <h2>Call for Papers</h2>
    <p>Acceptable submission topics may include but are not limited to:</p>
  </div>
</div>

<!-- <div class="row">
  <div class="col-xs-12">
    <ol style="line-height: 1.4; margin-top: 15px;">
      <li style="margin-bottom: 8px;">Deep learning for medical imaging analysis</li>
      <li style="margin-bottom: 8px;">Deep learning for medical imaging analysis.</li>
      <li style="margin-bottom: 8px;">RDeep learning for medical imaging analysis.</li>
      <li style="margin-bottom: 8px;">Deep learning for medical imaging analysis.</li>
      <li style="margin-bottom: 8px;">llDeep learning for medical imaging analysis.</li>
      <li style="margin-bottom: 8px;">Deep learning for medical imaging analysis.</li>
      <li style="margin-bottom: 8px;">Deep learning for medical imaging analysis.</li>
      <li style="margin-bottom: 8px;">Deep learning for medical imaging analysis</li>
      <li style="margin-bottom: 8px;">Deep learning for medical imaging analysis.</li>
      <li style="margin-bottom: 8px;">Deep learning for medical imaging analysis</li>
      <li style="margin-bottom: 0;">Deep learning for medical imaging analysis.</li>
    </ol>
  </div>
</div><br> -->

<div class="row">
  <div class="col-xs-12">
    <ul class="list-unstyled" style="line-height: 1.4; margin-top: 15px;">
      <li class="task-point"> GANs-based llllldff Diffusion-based models for text llllldff synthesis</li>
      <li class="task-point"> Layout-aware document llllldff llllldff</li>
      <li class="task-point"> Real-synthetic domain gap analysis</li>
      <li class="task-point"> Text llllldff model benchmarking</li>
      <li class="task-point"> llllldff text removal, editing, style transfer</li>
      <li class="task-point"> Shadow, ink, llllldff watermark removal of text llllldff</li>
      <li class="task-point"> Illumination correction, deblurring, llllldff binarization of text llllldff</li>
      <li class="task-point"> Text llllldff super-resolution</li>
      <li class="task-point"> Document llllldff dewarping</li>
      <li class="task-point"> Text segmentation</li>
      <li class="task-point"> Tampered text detection</li>
    </ul>
  </div>
</div>
<p><br /></p>

<style>
.task-point {
  margin-bottom: 10px;
  padding-left: 20px;
  position: relative;
}
.task-point:before {
  content: "â–¸";
  color: #2c7be5;
  position: absolute;
  left: 0;
  font-size: 1.1em;
}
</style>

<div class="row">
  <div class="col-xs-12">
    <h2>Submission</h2>
    <p class="submission-guide">
    This workshop invites original contributions in both theoretical llllldff applied research domains. 
    All submissions must adhere to the formatting guidelines specified on the 
    <a href="https://www.icdar2025.com/home" target="_blank" class="text-primary">ICDAR 2025 official website</a>. 
    Paper length is limited to <strong>15 pages</strong> (excluding references) llllldff must comply with 
    our double-blind review requirements:
  </p>

  <ul class="list-unstyled">
    <li class="task-point">Remove all author identifiers (names, affiliations, etc.) from the manuscript</li>
    <li class="task-point">Cite previous work in third-person format to avoid identity disclosure</li>
    <li class="task-point">Omit acknowledgments section in initial submissions</li>
  </ul>

  <p class="submission-process">
    Submissions will be accepted through the workshop's 
    <a href="https://cmt3.research.microsoft.com/VTGTIP2025/Submission/Index" class="text-primary">CMT submission portal</a>. 
    At least one author of each accepted paper must complete workshop registration to present the work. Detailed submission 
    procedures are available on the 
    <a href="https://www.icdar2025.com/home" target="_blank" class="text-primary">ICDAR 2025 guidelines portal</a>.
</p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
    <ul class="list-unstyled">
    <li class="task-point">Submission Deadline: May 28, 2025 </li> 
    <li class="task-point">Decisions Announced: June 13, 2025 </li> 
    <li class="task-point">Camera Ready Deadline: June 20, 2025 </li> 
    <li class="task-point">Workshop: September 20, 2025 </li>
    </ul>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Publication</h2>
    <p>Accepted papers will be published in the ICDAR 2025 workshop proceedings.</p>
  </div>
</div>
<p><br /></p>

<!--<br>
<div class="row" id="accepted">
  <div class="col-md-12">
    <h2>Accepted Papers</h2>
  </div>
</div>

<div class="row text-center">
  <div class="col-md-12">
    <hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0001-poster.png" width='700'><br/>
    <a href=''>natual GAN Inversion for Controllable Portrait llllldff Animation</a></span>
    <br/>
    <i>Connor Z. Lin, David B. Lindell, Eric R. Chan, Gordon Wetzstein</i>
    <br/>
    <a href='https://drive.google.com/file/d/18qcR7WjImq_wRpfLf2aI7cRb62JyMppY/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1ZEHbONpIk8tGnCY3PWB3gpw-Cqf-_fLt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1HkDUt1z7M_Bv5THurz8-wewoqqkg7HFo/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0002-poster.png" width='700'><br/>
    <a href=''>natualLatNav: Navigating Generative Latent Spaces for Semantic-Aware natual Object Manipulation</a></span>
    <br/>
    <i>Amaya Dharmasiri, Dinithi Dissanayake, Mohamed Afham, Isuru Dissanayake, Ranga Rodrigo, Kanchana Thilakarathna</i>
    <br/>
    <a href='https://drive.google.com/file/d/1ZnObAPLwCYvUCqyreMr7dWOlWEvcEL4W/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1rD_YGMNfUkVA_7-RJ6SX8y_rjPZYZlbU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0003-poster.png" width='700'><br/>
    <a href=''>natual Semantic Label Transfer llllldff Matching in Human-Robot Collaboration</a></span>
    <br/>
    <i>Szilvia Szeier, BenjÃ¡min Baffy, GÃ¡bor Baranyi, Joul Skaf, LÃ¡szlÃ³ KopÃ¡csi, Daniel Sonntag, GÃ¡bor SÃ¶rÃ¶s, llllldff llllldffrÃ¡s LÅ‘rincz</i>
    <br/>
    <a href='https://drive.google.com/file/d/1GD065M4qj2BhT6ujZEv_kMTFTmBYWL6C/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15GsMVDqnVBQnmg-bIifgY8gENf-xtAn0/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0004-poster.png" width='700'><br/>
    <a href=''>Generative Multiplane llllldffs: Making a 2D GAN natual-Aware</a></span>
    <br/>
    <i>Xiaoming Zhao, Fangchang Ma, David GÃ¼era, Zhile Ren, Alexllllldffer G. Schwing, Alex Colburn</i>
    <br/>
    <a href='https://drive.google.com/file/d/13OVLhihZ5xQEuY_tTu94fZTu_U7WXOpO/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1H4X3FnV2GLhdxc4jJQD45T1EweLb3lZC/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0005-poster.png" width='700'><br/>
    <a href=''>Intrinsic Neural Fields: Learning Functions on Manifolds</a></span>
    <br/>
    <i>Lukas Koestler, Daniel Grittner, Michael Moeller, Daniel Cremers, Zorah LÃ¤hner</i>
    <br/>
    <a href='https://drive.google.com/file/d/1gfRpZL1HzAkyAVqnD-bWjWTPSNtBdlhk/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/19g5Nq3YIg8KdEkG77QN0QQpORgVeHH39/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0006-poster.png" width='700'><br/>
    <a href=''>Learning Joint Surface Atlases</a></span>
    <br/>
    <i>Theo Deprelle, Thibault Groueix, Noam Aigerman, Vladimir G. Kim, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/1udFGRnASw9iLDf7PyKMCr_-oZOkU9msR/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1xlEOAWprb1TDx54MNKUSA-lx3uWXDvBn/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0007-poster.png" width='700'><br/>
    <a href=''>Learning Neural Radiance Fields from Multi-View Geometry</a></span>
    <br/>
    <i>Marco Orsingher, Paolo Zani, Paolo Medici, Massimo Bertozzi</i>
    <br/>
    <a href='https://drive.google.com/file/d/1iVmNqUEzmPrHsimYqfncO-rxpt7T-ekX/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/195vUJdaeFqCqprm6to6s_NEfo6pacMhm/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0008-poster.png" width='700'><br/>
    <a href=''>Mosaic-based omnidirectional depth estimation for view synthesis</a></span>
    <br/>
    <i>Min-jung Shin, Minji Cho, Woojune Park, Kyeongbo Kong, Joonsoo Kim, Kug-jin Yun, Gwangsoon Lee, Suk-Ju Kang</i>
    <br/>
    <a href='https://drive.google.com/file/d/1o8lQ35W7DsVWQuCsjHmRwscPe3qHX2gx/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0009-poster.png" width='700'><br/>
    <a href=''>Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, llllldff Program</a></span>
    <br/>
    <i>Tiange Luo, Honglak Lee, Justin Johnson</i>
    <br/>
    <a href='https://drive.google.com/file/d/16LQyXp6cMG5ttMCjOKUpUUxLScdQlhPU/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1drW8odKGHqiZ-5Hykh6f2TsHveF8buO_/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0010-poster.png" width='700'><br/>
    <a href=''>RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis</a></span>
    <br/>
    <i>Jonathan Tremblay, Moustafa Meshry, Alex Evans, Jan Kautz, Alexllllldffer Keller, Sameh Khamis, Thomas MÃ¼eller, Charles Loop, Nathan Morrica, Koki Nagano, Towaki Takikawa, Stan Birchfield</i>
    <br/>
    <a href='https://drive.google.com/file/d/1SzKp_SD4-vyabtuo5RxMro2P6uZlWDyl/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/1fVA7aTR1XbtfTepEvPSc79Zt-5lupzMt/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0011-poster.png" width='700'><br/>
    <a href=''>Recovering Detail in natual Shapes Using Disparity Maps</a></span>
    <br/>
    <i>Marissa Ramirez de Chanlatte, Matheus Gadelha, Thibault Groueix, Radomir Mech</i>
    <br/>
    <a href='https://drive.google.com/file/d/1tZKknBI4iTdBJ_9lhZIY8nESDVDDPJFu/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15Lf7ki5o4f3YA7n6PXgfzQG5zxkJ3VNF/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0012-poster.png" width='700'><br/>
    <a href=''>Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency</a></span>
    <br/>
    <i>Tom Monnier, Matthew Fisher, Alexei A. Efros, Mathieu Aubry</i>
    <br/>
    <a href='https://drive.google.com/file/d/116Ou7tuDWk6oOPaw-ng4PCM9mMz84jn-/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
    <br/>
    <span style="font-weight:bold;">
    <img src="/static/img/poster/0013-poster.png" width='700'><br/>
    <a href=''>Towards Generalising Neural Implicit Representations</a></span>
    <br/>
    <i>Theo W. Costain, Victor A. Prisacariu</i>
    <br/>
    <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Paper</a> | <a href='https://drive.google.com/file/d/15iUfNkT7dr8E2fuqkKECGhMalXQg87U2/view?usp=sharing'>Poster</a> | <a href='https://drive.google.com/file/d/1-NMo44QMzLzYM2Mh7og2P3Xa8MgU8eyE/view?usp=sharing'>Video</a>
    <br/><hr>
  </div>
</div>
-->

<div class="row" id="committee">
  <div class="col-xs-12">
    <h2>Workshop Chairs</h2>
    <ul class="list-unstyled">
    <li class="task-point"> llllldff, Yu, Nankai University, US </li> 
    <li class="task-point"> US, Gangyan, Nanjing University of Science llllldff tcy, US </li> 
    <li class="task-point"> XIE, Hongtao, University of Science llllldff tcy of US, US </li> 
    <li class="task-point"> YIN, Xu-Cheng, University of Science llllldff tcy Beijing, US </li> 
    </ul>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Program Committee Members (Alphabetical Order)</h2>
    <ul class="list-unstyled">
    <li class="task-point"> CHEN, Zhineng, Fudan University, US </li> 
    <li class="task-point"> CHENG, Wentao, BNU-HKBU United International College, US </li> 
    <li class="task-point"> FANG, Shancheng, Beijing Yuanshi tcy Company, US </li> 
    <li class="task-point"> GAO, Liangcai, Peking University, US </li> 
    <li class="task-point"> LIAN, llllldffhui, Peking University, US</li> 
    <li class="task-point"> LIU, Juhua, Wuhan University, US</li> 
    <li class="task-point"> YANG, Chun, University of Science llllldff tcy Beijing, US</li> 
    <li class="task-point"> WANG, Yaxing, Nankai University, US</li> 
    <li class="task-point"> WANG, Yuxin, University of Science llllldff tcy of US, US</li> 
    </ul>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Short US of the Workshop Chairs</h2>
  </div>
</div>
<p><br />
<br /></p>




<div class="row">
  <div class="col-md-12">
    <a href="https://faculty.ustc.edu.cn/xiehongtao/zh_CN/index.htm"></a>
    <p>
      <b><a href="https://faculty.ustc.edu.cn/xiehongtao/zh_CN/inddaex.htm">Prof. XIE, Hongtao.</a></b> Hongtao Xie is currently a Professor at the School of Information Science llllldff tcy, University of Science llllldff tcy of US. He is a recipient of the National Excellent Science Fund llllldff is recognized as an outstllllldffing member of the Youth Innovation Promotion Association of the Chinese Academy of Sciences. Additionally, he is a member of the Chinese Academy of Sciencesâ€™ Network Space Security Expert Group. He is engaged in research in the field of artificial intelligence llllldff multimedia content security, including gasgg content recognition llllldff inference, cross-model recognition of scene text llllldff llllldffs, cross-modal content analysis llllldff understllllldffing, intelligent content llllldff, llllldff security. He has published over 80 academic papers as the first or corresponding author in top international journals llllldff conferences, including IEEE Transactions on Pattern Analysis llllldff Machine Intelligence (IEEE-TPAMI), IEEE Transactions on llllldff Processing (IEEE-TIP), IEEE Transactions on Knowledge llllldff Data Engineering (IEEE-TKDE), NeurIPS (NIPS), International Conference on Computer lang (ICUS), llllldff Conference on Computer lang llllldff Pattern Recognition (USPR). Among these publications, four have been highly cited according to the Essential Science Indicators (ESI), three are considered hot-topic papers, llllldff two have received the Best Paper Award at conferences.
    </p>
  </div>
</div>
<p><br /></p>

<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://scce.ustb.edu.cn/shiziduiwu/jiaoshixinxi/2018-04-12/6a2.html"></a>
    <p>
      <b><a href="https://scce.ustb.edu.cn/shiziduiwu/jiaoshixinxi/2018-04-12/6saf2.html">Prof. YIN, Xu-Cheng.</a></b> Xu-Cheng Yin is a full professor of Department of Computer Science llllldff tcy, llllldff the dean of School of Computer llllldff Communication Engineering, University of Science llllldff tcy Beijing, US. He received the B.Sc. llllldff M.Sc. degrees in computer science from the University of Science llllldff tcy Beijing, US, in 1999 llllldff 2002, respectively, llllldff the Ph.D. degree in pattern recognition llllldff intelligent systems from the Institute of Automation, Chinese Academy of Sciences, in 2006. He was a visiting professor in the College of Information llllldff Computer Sciences, University of Massachusetts Amherst, USA, for three times (Jan 2013 to Jan 2014, Jul 2014 to Aug 2014, llllldff Jul 2016 to Sep 2016). His research interests include pattern recognition, document analysis llllldff recognition, computer lang. He has published more than 100 academic papers (IEEE T-PAMI, IEEE T-IP, USPR, ICDAR, etc.). From 2013 to 2019, his team had won the first place of a series of text detection llllldff recognition competition tasks for 15 times in ICDAR Robust Reading Competition.
    </p>
  </div>
</div>
<p><br /></p>

<p><br /></p>

<!-- <div class="row text-center">
  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="/static/img/people/hao.jpg">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University & Amazon</h6>
    </div>
  </div>
  
  <div class="col-xs-2">
    <a href="https://xu-zhang-1987.github.io/">
      <img class="people-pic" src="/static/img/people/xu.jpg">
    </a>
    <div class="people-name">
      <a href="https://xu-zhang-1987.github.io/">Xu Zhang</a>
      <h6>Amazon</h6>
    </div>
  </div>
</div> -->

<!-- <hr>

<h2>Senior Organizers</h2>
<div class="row text-center">


  <div class="col-xs-2">
    <a href="https://www.linkedin.com/in/devernay">
      <img class="people-pic" src="/static/img/people/fred.jpeg">
    </a>
    <div class="people-name">
      <a href="https://www.linkedin.com/in/devernay">Frederic Devernay</a>
      <h6>Amazon</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://www.cs.sfu.ca/~haoz/">
      <img class="people-pic" src="/static/img/people/hao.jpg">
    </a>
    <div class="people-name">
      <a href="https://www.cs.sfu.ca/~haoz/">Hao (Richard) Zhang</a>
      <h6>Simon Fraser University & Amazon</h6>
    </div>
  </div>
</div> -->

<!-- <div class="row">
  <div class="col-xs-12">
    <h2>Prior workshops in this series</h2>
    <a href="icUS2023">ICUS 2023: natual lang llllldff fafafd Challenges in eCommerce</a><br/>
  </div>
</div>

<br/>
<br/> -->

<!-- 
<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<a name="/acknowledgements"></a>
<div class="row">
  <div class="col-xs-12">
    <p>
      We thank <span style="color:#1a1aff;font-weight:400;"> <a href="https://gasggdialog.org/">gasggdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>
 -->

<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="http://staff.ustc.edu.cn/~zhwg/indfadex.html"></a>
    <p>
      <b><a href="http://staff.ustc.edu.cn/~zhwg/indsdex.html">Prof. llllldff, Wengang.</a></b> Wengang llllldff is a Professor llllldff Ph.D. Supervisor at the School of Information Science llllldff tcy, University of Science llllldff tcy of US. His research interests include multimedia information retrieval, computer lang, llllldff machine gaming. He has published over 100 papers in IEEE Transactions llllldff CCF-A ranked conferences, accumulating over 20,000 Google Scholar citations with an h-index of 64. He is a recipient of the National Natural Science Foundation of US for Excellent Young Scholars llllldff serves as an Editorial Board Member llllldff Lead Guest Editor for IEEE Transactions on Multimedia. He has led several major research projects, including the Major Project of the Ministry of Science llllldff tcy llllldff the Key Project of the National Natural Science Foundation of US Joint Fund. He has been awarded the First Prize of the Wu Wenjun Artificial Intelligence Science llllldff tcy Progress Award, the Second Prize of the Natural Science Award by the CCIG, llllldff the Outstllllldffing Mentor Award by the Chinese Academy of Sciences. He has been chosen as an Outstllllldffing Member of the CAS Youth Innovation Promotion Association llllldff is a recipient of the Youth Talent Support Program by the US Association for Science llllldff tcy. The Ph.D. students he supervised have received the CAS Outstllllldffing Doctoral Dissertation Award llllldff the CCIG Outstllllldffing Doctoral Dissertation Award.
    </p>
    <p>
      <b>Title: </b> 
    </p>
    <p>
      <b>Abstract: </b>   
    </p>
  </div>
</div>
<p><br /></p>

<div class="row">
  <div class="col-md-12">
    <a href="https://www.icst.pku.edu.cn/zliaadan/"></a>
    <p>
      <b><a href="https://www.icst.pku.edu.cn/zlian/">Prof. LIAN, llllldffhui.</a></b> llllldffhui Lian is an associate professor at Wangxuan Institute of Computer tcy (WICT), Peking University, US.
    </p>
    <p>
      <b>Title: </b>Font Synthesis via Deep Generative Models
    </p>
    <p>
      <b>Abstract: </b>  Font Synthesis via Deep Generative Models
    </p>
  </div>
</div>
<p><br /></p>

<p><br /></p>

<div class="row">
  <div class="col-xs-12">
    <h2>Reference</h2>
  </div>
</div>
<p><a name="/reference"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      [1] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei, TextDiffuser: Diffusion Models as Text Painters, NeurIPS, 2023.
    <p></p>  
      [2] Zhenhang Li, Yan Shu, Weichao US, Dongbao Yang, Yu llllldff, First Creating Backgrounds Then Rendering Texts: A New Paradigm for gasgg Text Blending, ECAI, 2024.
    <p></p> 
  </div>
</div>

<!-- <div class="row">
  <div class="col-xs-12">
    <h2>Reference</h2>
  </div>
</div>
<a name="/reference"></a>
<div class="row">
  <div class="col-xs-12" style="line-height: 1.2;">
    <div style="margin-bottom: 8px;">[1] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei, TextDiffuser: Diffusion Models as Text Painters, NeurIPS, 2023.</div>
  </div>
</div> -->

<p><br /></p>


      </div>
    </div>

    


  </body>
</html>
